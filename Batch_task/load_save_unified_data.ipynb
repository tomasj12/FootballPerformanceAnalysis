{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the match data notebook\n",
    "\n",
    "**Purpose of the notebook:** The purpose of this notebook is to load and unified the data for the further aggregations. This is the necessary part for the data loading.\n",
    "\n",
    "**Input of the notebook:** The input data are raw match data.\n",
    "\n",
    "**Output of the notebook:** The output of this notebook is `delta` table. \n",
    "\n",
    "**Some notes:**:\n",
    "* The `spark.DataFrame` will always have notation `_df` at the end of the name of variable\n",
    "* the `pandas.DataFrame` will always have notation `_pd` at the end of the name of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the environment\n",
    "\n",
    "In this part, the environment is set. The set up is:\n",
    "\n",
    "* Loading the necessary python modules and helper functions\n",
    "* Setting the path to data and metadata\n",
    "* Initialize the spark session\n",
    "\n",
    "Other config, such as `spark` application name, path, where the final `delta` table will be saved, etc. are defined in `config.yaml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from delta import *\n",
    "from utils import plot_pitch, ball_inside_box, read_config\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/18 21:58:47 WARN Utils: Your hostname, tomas-Yoga-Slim-7-Pro-14ACH5-O resolves to a loopback address: 127.0.1.1; using 192.168.0.53 instead (on interface wlp1s0)\n",
      "22/07/18 21:58:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/tomas/.ivy2/cache\n",
      "The jars for the packages stored in: /home/tomas/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9d40605a-2081-4731-835b-d62aa817fbbb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 300ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9d40605a-2081-4731-835b-d62aa817fbbb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/11ms)\n",
      "22/07/18 21:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/18 21:58:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "app_name = config['spark_application']['spark_app_batch_name']\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(app_name) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the remaining ,,env'' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"g1059778_Data.jsonl\"\n",
    "meta_data_path = \"/home/tomas/Personal_projects/Aston_Villa/data/g1059778_Metadata.xml\"\n",
    "delta_player_path = config['batch']['delta_player_dir']\n",
    "delta_ball_path = config['batch']['delta_ball_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_match_data_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(raw_data_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match date: 2019-10-05\n",
      "Field dimension: (104.85, 67.97)\n"
     ]
    }
   ],
   "source": [
    "with open(meta_data_path,'r') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "match_metadata = BeautifulSoup(metadata,'xml')\n",
    "\n",
    "metadata_match_data = match_metadata.find('match').get('dtDate').split(' ')[0]\n",
    "field_x = float(match_metadata.find('match').get('fPitchXSizeMeters'))\n",
    "field_y = float(match_metadata.find('match').get('fPitchYSizeMeters'))\n",
    "metadata_field_dim = (field_x,field_y)\n",
    "\n",
    "print(f\"Match date: {metadata_match_data}\")\n",
    "print(f\"Field dimension: {metadata_field_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the data\n",
    "\n",
    "In this part, the unified dataset is created. It is needed to somehow extract the values from the each `array` column (`homePlayers`,`awayPlayers`, `ball`). \n",
    "\n",
    "**Note: This approach will lead to duplicated dataset. However, for just storing the ingested raw data, it does not matter.**\n",
    "\n",
    "In the cell below:\n",
    "* `wallClock` is transformed to seconds and then the date of the match is extracted (the unique key). This will help us for identify ieach match uniquely and also tables can be partiotned by this column.\n",
    "* Also we get the timestamp from `wallClock` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match_date_df = (\n",
    "    raw_match_data_df\n",
    "    .withColumn('match_date', F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd')) \n",
    "    .withColumn('match_timestamp',F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd HH:mm:ss:S'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------------+\n",
      "|wallClock    |match_date|match_timestamp      |\n",
      "+-------------+----------+---------------------+\n",
      "|1570284007331|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007371|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007411|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007451|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007491|2019-10-05|2019-10-05 16:00:07:0|\n",
      "+-------------+----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    match_date_df\n",
    "    .select('wallClock','match_date','match_timestamp')\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell bellow:\n",
    "* both of `homePlayers` and `awayPlayers` columns are exploded, which we get each value of this array.\n",
    "* Unfortunatelly, duplicates are created (e.g because of exploding, there is more rows for one player id and each row with this row associated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = ['period','frameIdx','gameClock','wallClock','live','lastTouch','match_date']\n",
    "\n",
    "unified_players_df = (\n",
    "    match_date_df\n",
    "    .withColumn('home_players_exploded',F.explode('homePlayers')) \n",
    "    .withColumn('away_players_exploded',F.explode('awayPlayers'))\n",
    "    .select(\n",
    "        F.col('home_players_exploded.playerId').alias('homePlayer_playerId'),\n",
    "        F.col('home_players_exploded.speed').alias('homePlayer_speed'),\n",
    "        F.col('home_players_exploded.xyz').alias('homePlayer_3d_position'),\n",
    "        F.col('away_players_exploded.playerId').alias('awayPlayer_playerId'),\n",
    "        F.col('away_players_exploded.speed').alias('awayPlayer_speed'),\n",
    "        F.col('away_players_exploded.xyz').alias('awayPlayer_3d_position'),\n",
    "        *base_columns\n",
    "    )\n",
    "    .withColumn(\"home_player_3d_position_x\", F.col('homePlayer_3d_position').getItem(0))\n",
    "    .withColumn(\"home_player_3d_position_y\", F.col('homePlayer_3d_position').getItem(1))\n",
    "    .withColumn(\"home_player_3d_position_z\", F.col('homePlayer_3d_position').getItem(2))\n",
    "    .withColumn(\"away_player_3d_position_x\", F.col('awayPlayer_3d_position').getItem(0))\n",
    "    .withColumn(\"away_player_3d_position_y\", F.col('awayPlayer_3d_position').getItem(1))\n",
    "    .withColumn(\"away_player_3d_position_z\", F.col('awayPlayer_3d_position').getItem(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's make a unified data for the ball data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_ball_df = (\n",
    "    match_date_df\n",
    "    .select(\n",
    "        F.col(\"ball.xyz\").alias(\"ballPosition\"),\n",
    "        F.col(\"ball.speed\").alias(\"ballSpeed\"),\n",
    "        *base_columns\n",
    "    )\n",
    "    .withColumn(\"ballInsideBox\",ball_inside_box(F.col('ballPosition'),F.lit(\"inside_box\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is needed to first check, if the path to delta table exists. If there exists a partition in this folder, rows will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(delta_player_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_player_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_players_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_players_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_player_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "22/07/18 21:59:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/07/18 21:59:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(delta_ball_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_ball_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_ball_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_ball_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_ball_path)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
