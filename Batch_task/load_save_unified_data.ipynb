{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the match data notebook\n",
    "\n",
    "**Purpose of the notebook:** The purpose of this notebook is to load and unified the data for the further aggregations. This is the necessary part for the data loading.\n",
    "\n",
    "**Input of the notebook:** The input data are raw match data.\n",
    "\n",
    "**Output of the notebook:** The output of this notebook is `delta` table. \n",
    "\n",
    "**Some notes:**:\n",
    "* The `spark.DataFrame` will always have notation `_df` at the end of the name of variable\n",
    "* the `pandas.DataFrame` will always have notation `_pd` at the end of the name of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the environment\n",
    "\n",
    "In this part, the environment is set. The set up is:\n",
    "\n",
    "* Loading the necessary python modules and helper functions\n",
    "* Setting the path to data and metadata\n",
    "* Initialize the spark session\n",
    "\n",
    "Other config, such as `spark` application name, path, where the final `delta` table will be saved, etc. are defined in `config.yaml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from delta import *\n",
    "from utils import plot_pitch, ball_inside_box, read_config\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = config['spark_application']['spark_app_batch_name']\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(app_name) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the remaining ,,env'' variables\n",
    "\n",
    "Please, replace data paths for your data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"g1059778_Data.jsonl\"\n",
    "meta_data_path = \"/home/tomas/Personal_projects/Aston_Villa/data/g1059778_Metadata.xml\"\n",
    "delta_player_path = config['batch']['delta_player_dir']\n",
    "delta_ball_path = config['batch']['delta_ball_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'StructType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=0'>1</a>\u001b[0m raw_match_data_df \u001b[39m=\u001b[39m (\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=1'>2</a>\u001b[0m     spark\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49mread\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49mjson(raw_data_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mschema(assumed_schema)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomas/Personal_projects/Aston_Villa/Batch_task/load_save_unified_data.ipynb#ch0000011?line=5'>6</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'StructType' object is not callable"
     ]
    }
   ],
   "source": [
    "raw_match_data_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(raw_data_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match date: 2019-10-05\n",
      "Field dimension: (104.85, 67.97)\n"
     ]
    }
   ],
   "source": [
    "with open(meta_data_path,'r') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "match_metadata = BeautifulSoup(metadata,'xml')\n",
    "\n",
    "metadata_match_data = match_metadata.find('match').get('dtDate').split(' ')[0]\n",
    "field_x = float(match_metadata.find('match').get('fPitchXSizeMeters'))\n",
    "field_y = float(match_metadata.find('match').get('fPitchYSizeMeters'))\n",
    "metadata_field_dim = (field_x,field_y)\n",
    "\n",
    "print(f\"Match date: {metadata_match_data}\")\n",
    "print(f\"Field dimension: {metadata_field_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the data\n",
    "\n",
    "In this part, the unified dataset is created. It is needed to somehow extract the values from the each `array` column (`homePlayers`,`awayPlayers`, `ball`). \n",
    "\n",
    "**Note: This approach will lead to duplicated dataset. However, for just storing the ingested raw data, it does not matter.**\n",
    "\n",
    "In the cell below:\n",
    "* `wallClock` is transformed to seconds and then the date of the match is extracted (the unique key). This will help us for identify ieach match uniquely and also tables can be partiotned by this column.\n",
    "* Also we get the timestamp from `wallClock` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match_date_df = (\n",
    "    raw_match_data_df\n",
    "    .withColumn('match_date', F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd')) \n",
    "    .withColumn('match_timestamp',F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd HH:mm:ss:S'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------------+\n",
      "|wallClock    |match_date|match_timestamp      |\n",
      "+-------------+----------+---------------------+\n",
      "|1570284007331|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007371|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007411|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007451|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007491|2019-10-05|2019-10-05 16:00:07:0|\n",
      "+-------------+----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    match_date_df\n",
    "    .select('wallClock','match_date','match_timestamp')\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell bellow:\n",
    "* both of `homePlayers` and `awayPlayers` columns are exploded, which we get each value of this array.\n",
    "* Unfortunatelly, duplicates are created (e.g because of exploding, there is more rows for one player id and each row with this row associated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = ['period','frameIdx','gameClock','wallClock','live','lastTouch','match_date']\n",
    "\n",
    "unified_players_df = (\n",
    "    match_date_df\n",
    "    .withColumn('home_players_exploded',F.explode('homePlayers')) \n",
    "    .withColumn('away_players_exploded',F.explode('awayPlayers'))\n",
    "    .select(\n",
    "        F.col('home_players_exploded.playerId').alias('homePlayer_playerId'),\n",
    "        F.col('home_players_exploded.speed').alias('homePlayer_speed'),\n",
    "        F.col('home_players_exploded.xyz').alias('homePlayer_3d_position'),\n",
    "        F.col('away_players_exploded.playerId').alias('awayPlayer_playerId'),\n",
    "        F.col('away_players_exploded.speed').alias('awayPlayer_speed'),\n",
    "        F.col('away_players_exploded.xyz').alias('awayPlayer_3d_position'),\n",
    "        *base_columns\n",
    "    )\n",
    "    .withColumn(\"home_player_3d_position_x\", F.col('homePlayer_3d_position').getItem(0))\n",
    "    .withColumn(\"home_player_3d_position_y\", F.col('homePlayer_3d_position').getItem(1))\n",
    "    .withColumn(\"home_player_3d_position_z\", F.col('homePlayer_3d_position').getItem(2))\n",
    "    .withColumn(\"away_player_3d_position_x\", F.col('awayPlayer_3d_position').getItem(0))\n",
    "    .withColumn(\"away_player_3d_position_y\", F.col('awayPlayer_3d_position').getItem(1))\n",
    "    .withColumn(\"away_player_3d_position_z\", F.col('awayPlayer_3d_position').getItem(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's make a unified data for the ball data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_ball_df = (\n",
    "    match_date_df\n",
    "    .select(\n",
    "        F.col(\"ball.xyz\").alias(\"ballPosition\"),\n",
    "        F.col(\"ball.speed\").alias(\"ballSpeed\"),\n",
    "        *base_columns\n",
    "    )\n",
    "    .withColumn(\"ballInsideBox\",ball_inside_box(F.col('ballPosition'),F.lit(\"inside_box\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save player data\n",
    "\n",
    "Save unified player data as delta table in `delta_player_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(delta_player_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_player_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_players_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_players_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_player_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ball data\n",
    "\n",
    "Save the unified ball data as delta table into `delta_ball_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(delta_ball_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_ball_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_ball_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_ball_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_ball_path)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
