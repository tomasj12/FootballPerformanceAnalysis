{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the match data notebook\n",
    "\n",
    "**Purpose of the notebook:** The purpose of this notebook is to load and unified the data for the further aggregations. This is the necessary part for the data loading.\n",
    "\n",
    "**Input of the notebook:** The input data are raw match data.\n",
    "\n",
    "**Output of the notebook:** The output of this notebook is `delta` table. \n",
    "\n",
    "**Some notes:**:\n",
    "* The `spark.DataFrame` will always have notation `_df` at the end of the name of variable\n",
    "* the `pandas.DataFrame` will always have notation `_pd` at the end of the name of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the environment\n",
    "\n",
    "In this part, the environment is set. The set up is:\n",
    "\n",
    "* Loading the necessary python modules and helper functions\n",
    "* Setting the path to data and metadata\n",
    "* Initialize the spark session\n",
    "\n",
    "Other config, such as `spark` application name, path, where the final `delta` table will be saved, etc. are defined in `config.yaml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from delta import *\n",
    "from utils import plot_pitch, ball_inside_box, read_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/18 15:33:21 WARN Utils: Your hostname, tomas-Yoga-Slim-7-Pro-14ACH5-O resolves to a loopback address: 127.0.1.1; using 192.168.0.53 instead (on interface wlp1s0)\n",
      "22/07/18 15:33:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/tomas/.ivy2/cache\n",
      "The jars for the packages stored in: /home/tomas/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b8749fc8-aae3-4cf7-9f96-c8bed02a3d4b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 306ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b8749fc8-aae3-4cf7-9f96-c8bed02a3d4b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/14ms)\n",
      "22/07/18 15:33:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/18 15:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/07/18 15:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/07/18 15:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/07/18 15:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "app_name = config['spark_application']['spark_app_batch_name']\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(app_name) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the remaining ,,env'' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"g1059778_Data.jsonl\"\n",
    "meta_data_path = \"/home/tomas/Personal_projects/Aston_Villa/data/g1059778_Metadata.xml\"\n",
    "delta_player_path = config['batch']['delta_player_dir']\n",
    "delta_ball_path = config['batch']['delta_ball_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_match_data_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(raw_data_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match date: 2019-10-05\n",
      "Field dimension: (104.85, 67.97)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('/home/tomas/Personal_projects/Aston_Villa/data/g1059778_Metadata.xml','r') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "match_metadata = BeautifulSoup(metadata,'xml')\n",
    "\n",
    "metadata_match_data = match_metadata.find('match').get('dtDate').split(' ')[0]\n",
    "field_x = float(match_metadata.find('match').get('fPitchXSizeMeters'))\n",
    "field_y = float(match_metadata.find('match').get('fPitchYSizeMeters'))\n",
    "metadata_field_dim = (field_x,field_y)\n",
    "\n",
    "print(f\"Match date: {metadata_match_data}\")\n",
    "print(f\"Field dimension: {metadata_field_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the data\n",
    "\n",
    "In this part, the unified dataset is created. It is needed to somehow extract the values from the each `array` column (`homePlayers`,`awayPlayers`, `ball`). \n",
    "\n",
    "**Note: This approach will lead to duplicated dataset. However, for just storing the ingested raw data, it does not matter.**\n",
    "\n",
    "In the cell below:\n",
    "* `wallClock` is transformed to seconds and then the date of the match is extracted (the unique key). This will help us for identify ieach match uniquely and also tables can be partiotned by this column.\n",
    "* Also we get the timestamp from `wallClock` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match_date_df = (\n",
    "    raw_match_data_df\n",
    "    .withColumn('match_date', F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd')) \n",
    "    .withColumn('match_timestamp',F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd HH:mm:ss:S'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------------+\n",
      "|wallClock    |match_date|match_timestamp      |\n",
      "+-------------+----------+---------------------+\n",
      "|1570284007331|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007371|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007411|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007451|2019-10-05|2019-10-05 16:00:07:0|\n",
      "|1570284007491|2019-10-05|2019-10-05 16:00:07:0|\n",
      "+-------------+----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    match_date_df\n",
    "    .select('wallClock','match_date','match_timestamp')\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell bellow:\n",
    "* both of `homePlayers` and `awayPlayers` columns are exploded, which we get each value of this array.\n",
    "* Unfortunatelly, duplicates are created (e.g because of exploding, there is more rows for one player id and each row with this row associated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = ['period','frameIdx','gameClock','wallClock','live','lastTouch','match_date']\n",
    "\n",
    "unified_players_df = (\n",
    "    match_date_df\n",
    "    .withColumn('home_players_exploded',F.explode('homePlayers')) \n",
    "    .withColumn('away_players_exploded',F.explode('awayPlayers'))\n",
    "    .select(\n",
    "        F.col('home_players_exploded.playerId').alias('homePlayer_playerId'),\n",
    "        F.col('home_players_exploded.speed').alias('homePlayer_speed'),\n",
    "        F.col('home_players_exploded.xyz').alias('homePlayer_3d_position'),\n",
    "        F.col('away_players_exploded.playerId').alias('awayPlayer_playerId'),\n",
    "        F.col('away_players_exploded.speed').alias('awayPlayer_speed'),\n",
    "        F.col('away_players_exploded.xyz').alias('awayPlayer_3d_position'),\n",
    "        *base_columns\n",
    "    )\n",
    "    .withColumn(\"home_player_3d_position_x\", F.col('homePlayer_3d_position').getItem(0))\n",
    "    .withColumn(\"home_player_3d_position_y\", F.col('homePlayer_3d_position').getItem(1))\n",
    "    .withColumn(\"home_player_3d_position_z\", F.col('homePlayer_3d_position').getItem(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's make a unified data for the ball data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_ball_df = (\n",
    "    match_date_df\n",
    "    .select(\n",
    "        F.col(\"ball.xyz\").alias(\"ballPosition\"),\n",
    "        F.col(\"ball.speed\").alias(\"ballSpeed\"),\n",
    "        *base_columns\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is needed to first check, if the path to delta table exists. If there exists a partition in this folder, rows will be updated. The reason can be the data are changed, e.g or new column is added (TODO: toto si este musim overit :D )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(delta_player_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_player_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_players_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_players_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_player_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(delta_ball_path):\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark, delta_ball_path)\n",
    "\n",
    "        (\n",
    "            deltaTable.alias('oldData')\n",
    "            .merge(\n",
    "                unified_ball_df.alias('newData'),\n",
    "                \"oldData.match_date = newData.match_date\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "else:\n",
    "\n",
    "    (\n",
    "        unified_ball_df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .partitionBy('match_date')\n",
    "        .save(delta_player_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Grouping:\n",
    "\n",
    "Podme to urobit co najjednoduchsie:\n",
    "* urobit dva dataframey (vzdy to bdue v dany zapas away team a home team, zgrupovat dva dataframey a dat do jedneho, kde bude identifikator, ci ide o home/away team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu by sa pocitali vlastne tie statistiky a performance jednotlivych hracov. uz ale z nacitaneho datasetu - resp. podla mna by mala existovat dalsia tabulka, aby sa to historizovalo.\n",
    "\n",
    "Ja by som si kludne niekde nechal tie data, a potom si pocital tieto agregacie a spolu s datumom..cize nejaky feature store ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowTop = Window.partitionBy(\"away_home_team\").orderBy(F.col(\"player_avg_speed\").desc())\n",
    "\n",
    "columns = unified_df.columns\n",
    "\n",
    "home_columns = [col for col in columns if col.startswith('home')] + base_columns\n",
    "home_df = (\n",
    "    unified_df\n",
    "    .select(home_columns)\n",
    ")\n",
    "\n",
    "home_df_grouped = (\n",
    "    home_df\n",
    "    .groupBy('homePlayer_playerId')\n",
    "    .agg(\n",
    "        F.avg('homePlayer_speed').alias('player_avg_speed'),\n",
    "        F.max('homePlayer_speed').alias('player_max_speed')\n",
    "    )\n",
    "    .withColumn('away_home_team',F.lit('home'))\n",
    "    .withColumnRenamed('homePlayer_playerId','playerId')\n",
    ")\n",
    "\n",
    "away_columns = [col for col in columns if col.startswith('away')] + base_columns\n",
    "\n",
    "away_df = (\n",
    "    unified_df\n",
    "    .select(away_columns)\n",
    ")\n",
    "\n",
    "away_df_grouped = (\n",
    "    away_df\n",
    "    .groupBy('awayPlayer_playerId')\n",
    "    .agg(\n",
    "        F.avg('awayPlayer_speed').alias('player_avg_speed'),\n",
    "        F.max('awayPlayer_speed').alias('player_max_speed')\n",
    "    )\n",
    "    .withColumn('away_home_team',F.lit('away'))\n",
    "    .withColumnRenamed('awayPlayer_playerId','playerId')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_performance_df = (\n",
    "    home_df_grouped\n",
    "    .union(away_df_grouped)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------------+--------------+----+\n",
      "|playerId|player_avg_speed  |player_max_speed|away_home_team|rank|\n",
      "+--------+------------------+----------------+--------------+----+\n",
      "|230046  |2.0587558851714207|7.43            |away          |1   |\n",
      "|193488  |1.949944498725782 |9.09            |away          |2   |\n",
      "|85242   |1.9467171484365127|9.03            |away          |3   |\n",
      "|193111  |2.0908897298961997|8.31            |home          |1   |\n",
      "|87396   |2.0776437014192446|8.5             |home          |2   |\n",
      "|124165  |2.0078769219424277|7.66            |home          |3   |\n",
      "+--------+------------------+----------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    players_performance_df\n",
    "    .orderBy('playerId')\n",
    "    .withColumn('rank',F.row_number().over(windowTop))\n",
    "    .filter(F.col('rank') <= 3)\n",
    ").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/15 23:45:13 WARN BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\n",
      "22/07/15 23:45:13 WARN BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization after Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\n",
      "22/07/15 23:45:18 WARN MergeIntoCommand: Merge source has SQLMetric(id: 1338, name: Some(number of source rows), value: 153972) rows in initial scan but SQLMetric(id: 1339, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n"
     ]
    }
   ],
   "source": [
    "# insert new mapping\n",
    "\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, './test/match_one')\n",
    "\n",
    "(\n",
    "    deltaTable.alias('oldData')\n",
    "    .merge(\n",
    "        newData.alias('newData'),\n",
    "        \"oldData.match_date = newData.match_date\"\n",
    "    )\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_perf = (\n",
    "    temp_df\n",
    "    .withColumn('ball_seconds',F.when(F.col('ball_inside_box') == True, 0.04).otherwise(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Okay toto zatial neham tak, treba si teraz rozmysliet tu rychlost hraca po x-ovej osi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|(sum(ball_seconds) / 60)|\n",
      "+------------------------+\n",
      "|4.341999999999955       |\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ball_perf.agg(F.sum('ball_seconds')/60).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------------+-------------------+----------------+----------------------+------+--------+---------+-------------+-----+---------+----------+---------------+-------------------------+-------------------------+-------------------------+\n",
      "|homePlayer_playerId|homePlayer_speed|homePlayer_3d_position|awayPlayer_playerId|awayPlayer_speed|awayPlayer_3d_position|period|frameIdx|gameClock|wallClock    |live |lastTouch|match_date|ball_inside_box|home_player_3d_position_x|home_player_3d_position_y|home_player_3d_position_z|\n",
      "+-------------------+----------------+----------------------+-------------------+----------------+----------------------+------+--------+---------+-------------+-----+---------+----------+---------------+-------------------------+-------------------------+-------------------------+\n",
      "|198826             |0.0             |[14.98, -4.01, 0.0]   |21205              |0.0             |[-52.29, 0.09, 0.0]   |1     |0       |0.0      |1570284007331|false|home     |2019-10-05|false          |14.98                    |-4.01                    |0.0                      |\n",
      "|198826             |0.0             |[14.98, -4.01, 0.0]   |203368             |0.0             |[-15.42, -14.3, 0.0]  |1     |0       |0.0      |1570284007331|false|home     |2019-10-05|false          |14.98                    |-4.01                    |0.0                      |\n",
      "|198826             |0.0             |[14.98, -4.01, 0.0]   |149484             |0.0             |[-21.72, 9.17, 0.0]   |1     |0       |0.0      |1570284007331|false|home     |2019-10-05|false          |14.98                    |-4.01                    |0.0                      |\n",
      "|198826             |0.0             |[14.98, -4.01, 0.0]   |85242              |0.0             |[-11.62, 8.74, 0.0]   |1     |0       |0.0      |1570284007331|false|home     |2019-10-05|false          |14.98                    |-4.01                    |0.0                      |\n",
      "|198826             |0.0             |[14.98, -4.01, 0.0]   |114283             |0.0             |[-9.22, 16.75, 0.0]   |1     |0       |0.0      |1570284007331|false|home     |2019-10-05|false          |14.98                    |-4.01                    |0.0                      |\n",
      "+-------------------+----------------+----------------------+-------------------+----------------+----------------------+------+--------+---------+-------------+-----+---------+----------+---------------+-------------------------+-------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unified_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|homePlayer_playerId|maximum_speed_x  |\n",
      "+-------------------+-----------------+\n",
      "|232980             |8.750000000007994|\n",
      "|195546             |8.250000000007505|\n",
      "|87396              |7.500000000006839|\n",
      "|71738              |8.500000000007732|\n",
      "|78607              |9.00000000000826 |\n",
      "+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowDelta = Window.partitionBy(\"homePlayer_playerId\").orderBy(\"period\",\"gameClock\")\n",
    "windowTimeDelta = Window.partitionBy(\"homePlayer_playerId\").orderBy(\"period\")\n",
    "\n",
    "(\n",
    "    home_df\n",
    "    .dropDuplicates()\n",
    "    .withColumn('x_pos_lag',F.lag(\"home_player_3d_position_x\",1).over(windowDelta))\n",
    "    .withColumn('y_pos_lag',F.lag(\"home_player_3d_position_y\",1).over(windowDelta))\n",
    "    .withColumn(\"time_lag\",F.lag(\"gameClock\").over(windowTimeDelta))\n",
    "    .fillna(0.0)\n",
    "    .withColumn(\"delta_x\",F.abs(F.col(\"home_player_3d_position_x\") - F.col(\"x_pos_lag\")))\n",
    "    .withColumn(\"delta_y\",F.abs(F.col(\"home_player_3d_position_y\") - F.col(\"y_pos_lag\")))\n",
    "    .withColumn(\"delta_time\",F.col(\"gameClock\") - F.col(\"time_lag\"))\n",
    "    .withColumn(\"speed_x\", F.col(\"delta_x\")/F.col(\"delta_time\"))\n",
    "    .withColumn(\"speed_y\", F.col(\"delta_y\")/F.col(\"delta_time\"))\n",
    "    .groupBy(\"homePlayer_playerId\")\n",
    "    .agg(F.max(\"speed_x\").alias(\"maximum_speed_x\"))\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uloha 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
