{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for reading the stream of the data\n",
    "\n",
    "**Purpose of the notebook:** The purpose of the notebook is to catch a stream data from the socket.\n",
    "\n",
    "**Input of the notebook:** Streamed data from the socket\n",
    "\n",
    "**Output of the notebook:** The parquet, where the streamed data are stored incrementally.\n",
    "\n",
    "**Some notes:**:\n",
    "* The `spark.DataFrame` will always have notation `_df` at the end of the name of variable\n",
    "* the `pandas.DataFrame` will always have notation `_pd` at the end of the name of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the environment\n",
    "\n",
    "In this part, the environment is set. The set up is:\n",
    "\n",
    "* Loading the necessary python modules and helper functions\n",
    "* Setting the path to data and metadata\n",
    "* Initialize the spark session\n",
    "\n",
    "Other config, such as `spark` application name, path, where the final `delta` table will be saved, etc. are defined in `config.yaml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from utils import init_spark_session, read_config,ball_inside_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_app_name = config['spark_application']['spark_app_stream_name']\n",
    "checkpoint = config['streaming']['file_checkpoint']\n",
    "file_location = config['streaming']['file_dir']\n",
    "termination = config['streaming']['wait_for_termination']\n",
    "file_format = config['streaming']['format']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark_session(spark_app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==========================================================\")\n",
    "print(\"Streaming has started!\")\n",
    "print(f\"Results are saved in {format} format in dir: {file_location}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_stream_df = (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"socket\")\n",
    "        .option('host','localhost')\n",
    "        .option('port','9898')\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do transformations on the streamed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = (\n",
    "        df\n",
    "        .withColumn('wallClock',F.split(\"value\",\":\").getItem(0))\n",
    "        .withColumn('match_timestamp',F.from_unixtime(F.col('wallClock')/1000, 'yyyy-MM-dd HH:mm:ss:SSS'))\n",
    "        .withColumn('playerAttributes',F.split(\"value\",\":\").getItem(1))\n",
    "        .withColumn('ballAttributes',F.split(\"value\",\":\").getItem(2))\n",
    "        .withColumn(\"ball_position_x\",F.split(\"ballAttributes\",\",\").getItem(0)/100)\n",
    "        .withColumn(\"ball_position_y\",F.split(\"ballAttributes\",\",\").getItem(1)/100)\n",
    "        .withColumn(\"ball_position_z\",F.split(\"ballAttributes\",\",\").getItem(2)/100)\n",
    "        .withColumn(\"ballPosition\",F.array('ball_position_x','ball_position_y','ball_position_z'))\n",
    "        .withColumn(\"ballStatus\",F.regexp_replace(F.split(\"ballAttributes\",\",\").getItem(5),\";\",\"\"))\n",
    "        .withColumn(\"ballSpeed\",F.split(\"ballAttributes\",\",\").getItem(3))\n",
    "        .withColumn(\"ballTeam\",F.split(\"ballAttributes\",\",\").getItem(4))\n",
    "        .withColumn(\"ballInsideBox\",ball_inside_box(\"ballPosition\",F.lit(\"inside_box\")))\n",
    "        .withColumn(\"ballInsideField\",ball_inside_box(\"ballPosition\",F.lit(\"inside_field\")))\n",
    "        .select(\"match_timestamp\",\"wallClock\",\"ballAttributes\",'ballPosition',\"ballStatus\",\"ballSpeed\",\"ballTeam\",\"ballInsideBox\",\"ballInsideField\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the streamed data incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    split_df\n",
    "    .writeStream\n",
    "    .format(file_format)\n",
    "    .option('checkpointLocation',checkpoint)\n",
    "    .option(\"path\",file_location)\n",
    "    .start()    \n",
    "    .awaitTermination()\n",
    "    .stop()\n",
    ")\n",
    "\n",
    "print(\"Streaming has finished\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
